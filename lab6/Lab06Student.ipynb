{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6a (Simple queue model).** Consider the following process: There is a queue that can hold up to $N$ people. At each timestep a person arrives with probability $p_a$, and then a person is served and leaves with probability $p_d$. If there are $N$ people in the queue, no new person arrives. Similarly, if there are no people in the queue, no person leaves. Write a function that, given $N$, $p_a$ and $p_d$ generates a **numpy** array representing the transition matrix of the corresponding Markov chain. Recall that the transition matrix $M$ has $M_{s,t}$ equal to the probability of going from state $s$ to state $t$ in a single step.\n",
    "\n",
    "The answer for $N=3$, $p_a=0.6$ and $p_d=0.4$ should look like this\n",
    "\n",
    "$$\\left( \\begin{array}{cccc}0.64 & 0.36 & 0.00 & 0.00 \\\\\n",
    "0.16 & 0.48 & 0.36 & 0.00 \\\\\n",
    "0.00 & 0.16 & 0.48 & 0.36 \\\\\n",
    "0.00 & 0.00 & 0.16 & 0.84 \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6b (Stationary distribution).** What happens in the long run? As stated in the lecture, for large $k$ the distribution over states after $k$ steps is (almost) independent of the starting state. But what is this distribution? One way to see this is to pick a starting distribution, say $\\pi_0 = (1,0,\\ldots,0)$, and compute $\\pi_k$ for a very large $k$ using the transition matrix $M$. \n",
    "\n",
    "Do this for $k=10\\,000$, $N=20$ and:\n",
    " * $p_a = 0.45$, $p_q=0.55$,\n",
    " * $p_a = 0.5$, $p_q=0.5$, and\n",
    " * $p_a = 0.55$, $p_q=0.45$ (you do not really need to do this, do you see why?).\n",
    " \n",
    "Discuss the results. Plotting $\\pi_k$ might be instructive here.\n",
    " \n",
    "**Hint:** The **numpy** function **linalg.matrix_power** might come in handy. Also to multiply two matrices (a vector is a matrix, too!) you can use **numpy.matmul**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6c (Rate of convergence).** You probably used the $k$-th power of $M$ in the previous problem. Inspect this matrix for $N=10$, $k=10\\,000$. Explain what you see.\n",
    "\n",
    "To see how quickly the chain converges to the stationary distribution $\\pi$, we can compute $M^k$ for different values of $k$ and look at how different are the rows of $M^k$ from $\\pi$. Do this for $k=1,\\ldots,1000$ and plot the results. To measure the distance compute the $l_1$ distance between each row of $M^k$ and $\\pi$ and pick the largest one. \n",
    "\n",
    "Use a logarithmic scale for the distance. Explain what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6d (Stationary distribution).** The stationary distribution $\\pi$ satisfies the equation $\\pi M = \\pi$, or equivalently $M^T \\pi^T = \\pi^T$. This means that $\\pi^T$ is an eigenvector of $M^T$ with eigenvalue $1$. It can be shown that for a transition matrix of a Markov Chain, under certain assumptions(see lecture and Perron-Frobenius theorem):\n",
    " * $1$ is the eigenvalue with largest absolute value,\n",
    " * $1$ has multiplicity one and there exists an eigenvector with all entries positive,\n",
    " \n",
    "Use this information to find the stationary distribution for $M$ and compare the results with the previous problem. You might want to use **numpy.linalg.eig** to find the vector $d$ of eigenvalues and a matrix $V$ with corresponding eigenvectors as columns. You can find the distribution $\\pi$ in the column of $V$ corresponding to the eigenvalue $1$.\n",
    "\n",
    "Note that $M^TV = V\\textrm{diag}(d)$, where $\\textrm{diag}(d)$ is a diagonal matrix with the eigenvalues at the diagonal. Therefore\n",
    "$$M^T = V\\textrm{diag}(d)V^{-1}.$$\n",
    "This is a so called *eigenvalue decomposition* for $M^T$. \n",
    " * How can you use this decomposition to quickly compute powers of $M^T$?\n",
    " * What happens when you compute a high power of $M^T$? What does that say about the convergence rate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
